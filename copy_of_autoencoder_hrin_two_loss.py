# -*- coding: utf-8 -*-
"""Copy of Autoencoder_HRIN_two_loss.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Y_cCkPXcfBB5XfJDiBva-MsVonNP1BDH
"""

from google.colab import drive
drive.mount('/content/drive')

!nvidia-smi

import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import *
from tensorflow.keras.callbacks import LearningRateScheduler, ModelCheckpoint
from sklearn.model_selection import train_test_split
from matplotlib import pyplot as plt
import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler,StandardScaler
from tensorflow.keras import optimizers,metrics
from tensorflow.keras.models import load_model
# from sklearn.externals import joblib

#%% Read dataset
data=pd.read_csv("/content/drive/MyDrive/dl/alpha_srin_32.csv")
y = pd.read_csv("/content/drive/MyDrive/dl/result_srin_32.csv")
#data.drop(['Rd','Rn','dn','dc'],axis = 1 , inplace = True)

y.isna().sum()
y["lc"].hist()
y

dt=data
print(dt)

scaler = MinMaxScaler(feature_range=(0,1))
dt = scaler.fit_transform(dt)

X_train, X_test, y_train, y_test = train_test_split(dt, y, test_size=0.2)
X_train = X_train.reshape(X_train.shape[0],X_train.shape[1],1)
X_test = X_test.reshape(X_test.shape[0],X_test.shape[1],1)

data_shape = X_test.shape
data_shape

def get_model(shape):
  x_in = Input((shape,1))
  x = Conv1D(filters=16, kernel_size=3, activation='relu')(x_in)
  x=  MaxPooling1D(2)(x)
  x = Conv1D(filters=32, kernel_size=3, activation='relu')(x)
  x=  MaxPooling1D(2)(x)
  x = Conv1D(filters=64, kernel_size=3, activation='relu')(x)
  x=  MaxPooling1D(2)(x)
  x = Conv1D(filters=128, kernel_size=3, activation='relu')(x)
  x=  MaxPooling1D(2)(x)
  x = Conv1D(filters=256, kernel_size=3, activation='relu')(x)
  x=  MaxPooling1D(2)(x)
  x = Flatten()(x)
  x = Dense(1024, activation='relu')(x)
  x = Dense(1024, activation='relu')(x)
  x = Dense(1024, activation='relu')(x)
  x = Dense(1024, activation='relu')(x)
  # x = Dropout(0.2)(x)
  x = out_1 = Dense(4, activation='linear')(x)
  x = Dense(1024, activation='relu')(x)
  x = Dense(95, activation='relu')(x)
  x=  Reshape((95,1))(x)
  # x = Conv1D(filters=256, kernel_size=5, activation='relu')(x)
  # x=  UpSampling1D(2)(x)
  x = Conv1D(filters=128, kernel_size=5, activation='relu')(x)
  x=  UpSampling1D(2)(x)
  x = Conv1D(filters=64, kernel_size=5, activation='relu')(x)
  x=  UpSampling1D(2)(x)
  x = Conv1D(filters=32, kernel_size=5, activation='relu')(x)
  x=  UpSampling1D(2)(x)
  out_2= x = Conv1D(filters=1, kernel_size=5, activation='relu')(x)
  # x=  Flatten()(x)
  # out_2 = Dense(shape,activation='linear')(x)
  return tf.keras.Model(x_in,[out_1,out_2])

shape = data_shape[1]
model = get_model(shape)

def loss_2(y_true,y_pred):
    return tf.keras.backend.mean(tf.keras.backend.abs(y_pred - y_true)*0.005)

mse_loss1=tf.keras.losses.MeanAbsoluteError()
mse_loss2=loss_2

losses = [mse_loss1,mse_loss2]

decay = 410*100 # number epochs
lr_func = tf.keras.optimizers.schedules.CosineDecay(initial_learning_rate=1e-5,decay_steps=decay, alpha=1e-6)

# Compile the network :
adam= optimizers.Adam(learning_rate=lr_func)
model.compile(loss=losses, optimizer=adam,metrics='mape')
model.summary()

# model.save('/content/drive/My Drive/dl/autoencoder.hdf5')

model.load_weights('/content/drive/My Drive/dl/autoencoder.hdf5')

checkpoint_name = '/content/drive/My Drive/dl/autoencoder_test.hdf5'
checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')
callbacks_list = [checkpoint]

history = model.fit(X_train,[y_train,X_train],validation_split=0.2, epochs=500, batch_size=500,verbose=1,callbacks=callbacks_list)

score=model.evaluate(X_test, [y_test,X_test], verbose=1)
print(score[0])

loss=history.history['loss']
mape=history.history['dense_4_mape']
val_loss=history.history['val_loss']
val_mape=history.history['val_dense_4_mape']
dict={'loss':loss,'mape':mape,'val_loss':val_loss,'val_mape':val_mape}
df= pd.DataFrame(dict)
df.to_csv('loss_can.csv', index=False)

encoder= tf.keras.models.Sequential()
for layer in model.layers[:-10]:
  encoder.add(layer)
encoder.summary()

adam= optimizers.Adam(learning_rate=1e-6,decay=1e-6)
encoder.compile(loss='mae', optimizer=adam,metrics='mape')
score=encoder.evaluate(X_test, y_test, verbose=1)
print(score[0])

predictions=encoder.predict(X_test)
print(predictions)

bv=pd.read_csv('encoder.csv')
#bv.drop('Rd',axis = 1 , inplace = True)

# scaler = joblib.load('scaler_320')

bv = scaler.transform(bv)

bv_n = np.expand_dims(bv,axis=2)
bv_n.shape

#  bv = bv.reshape(bv.shape[0],bv.shape[1],1)

valid=encoder.predict(bv_n)
valid

# model.save('/content/drive/My Drive/dl/all_hrin.hdf5')

csfont = {'fontname':'Serif'}
hfont = {'fontname':'Helvetica'}
plt.plot(predictions[0:25,0],label='Predicted',marker='o',markerfacecolor='blue',linestyle='-',color='blue')
plt.plot(np.array(y_test)[0:25,0],label='Actual',marker='*',markerfacecolor='red',linestyle='-',color='red')
plt.title('Neck length ($l_{n}$)',**csfont)
plt.legend(['Predicted', 'Actual'], loc='best')
plt.xlabel('Sample number',**csfont)
plt.ylabel('Value of $l_{n}$ ($m$)',**csfont)
plt.savefig('necklength.jpeg',dpi=600)
plt.show()
plt.plot(predictions[0:25,1],label='Predicted',marker='o',markerfacecolor='blue',linestyle='-',color='blue')
plt.plot(np.array(y_test)[0:25,1],label='Actual',marker='*',markerfacecolor='red',linestyle='-',color='red')
plt.title('Cavity length($l_{c}$)',**csfont)
plt.legend(['Predicted', 'Actual'], loc='best')
plt.xlabel('Sample number',**csfont)
plt.ylabel('Value of $l_{c}$ ($m$)',**csfont)
plt.savefig('cavitylength.jpeg',dpi=600)
plt.show()
plt.plot(predictions[0:25,2],label='Predicted',marker='o',markerfacecolor='blue',linestyle='-',color='blue')
plt.plot(np.array(y_test)[0:25,2],label='Actual',marker='*',markerfacecolor='red',linestyle='-',color='red')
plt.title('Neck radius ($r_{n}$)',**csfont)
plt.legend(['Predicted', 'Actual'], loc='best')
plt.xlabel('Sample number',**csfont)
plt.ylabel('Value of $r_{n}$ ($m$)',**csfont)
plt.savefig('neckradius.jpeg',dpi=600)
plt.show()
plt.plot(predictions[0:25,3],label='Predicted',marker='o',markerfacecolor='blue',linestyle='-',color='blue')
plt.plot(np.array(y_test)[0:25,3],label='Actual',marker='*',markerfacecolor='red',linestyle='-',color='red')
plt.title('Cavity radius ($r_{n}$)',**csfont)
plt.legend(['Predicted', 'Actual'], loc='best')
plt.xlabel('Sample number',**csfont)
plt.ylabel('Value of $r_{c}$ ($m$)',**csfont)
plt.savefig('cavityadius.jpeg',dpi=600)
plt.show()

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
font_axis_publish = {
        'color':  'black',
        'weight': 'bold',
        'size': 18,
        }
# plt.title('model loss')
plt.ylabel('loss',font_axis_publish)
plt.xlabel('epoch',font_axis_publish)
plt.legend(['Training loss', 'Validation loss'], loc='upper right',prop={'family': 'Serif'})
plt.savefig('loss.eps',dpi=600)
plt.show()

am=history.history['loss']
bm=history.history['val_loss']
df= pd.DataFrame(am)
df1 = pd.DataFrame(bm)
df.to_csv('loss.csv', index=False)
df1.to_csv('val.csv', index=False)

# #Build Model
# inp = Input(shape=(X_train.shape[1],1))
# C = Conv1D(filters=32, kernel_size=5, strides=1)(inp)

# C11 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(C)
# A11 = Activation("relu")(C11)
# C12 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(A11)
# S11 = Add()([C12, C])
# A12 = Activation("relu")(S11)
# M11 = MaxPooling1D(pool_size=5, strides=2)(A12)


# C21 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(M11)
# A21 = Activation("relu")(C21)
# C22 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(A21)
# S21 = Add()([C22, M11])
# A22 = Activation("relu")(S11)
# M21 = MaxPooling1D(pool_size=5, strides=2)(A22)


# C31 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(M21)
# A31 = Activation("relu")(C31)
# C32 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(A31)
# S31 = Add()([C32, M21])
# A32 = Activation("relu")(S31)
# M31 = MaxPooling1D(pool_size=5, strides=2)(A32)


# C41 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(M31)
# A41 = Activation("relu")(C41)
# C42 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(A41)
# S41 = Add()([C42, M31])
# A42 = Activation("relu")(S41)
# M41 = MaxPooling1D(pool_size=5, strides=2)(A42)


# C51 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(M41)
# A51 = Activation("relu")(C51)
# C52 = Conv1D(filters=32, kernel_size=5, strides=1, padding='same')(A51)
# S51 = Add()([C52, M41])
# A52 = Activation("relu")(S51)
# M51 = MaxPooling1D(pool_size=5, strides=2)(A52)

# F1 = Flatten()(M51)

# D1 = Dense(32)(F1)
# A6 = Activation("relu")(D1)
# D2 = Dense(32)(A6)
# D3 = Dense(1)(D2)

# model = Model(inputs=inp, outputs=D3)

dt['split'] = np.random.randn(dt.shape[0], 1)

msk = np.random.rand(len(dt)) <= 0.00001

train_new = dt[msk]
train_new.to_csv('train.csv', index=False)

model=load_model('/content/drive/My Drive/dl/all_hrin.hdf5')
model.summary()

import math

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn.metrics import r2_score

# Font for figure for publishing
font_axis_publish = {
        'color':  'black',
        'weight': 'bold',
        'size': 22,
        }
plt.rcParams['ytick.labelsize'] = 16
plt.rcParams['xtick.labelsize'] = 16

# Read in data
x = np.array(y_test)[0:500,0]
y = predictions[0:500,0]

# Plot Figures
fignow = plt.figure(figsize=(8,8))


## find the boundaries of X and Y values
bounds = (min(x.min(), y.min()) - int(0.1 * y.min()), max(x.max(), y.max())+ int(0.1 * y.max()))

# Reset the limits
ax = plt.gca()
ax.set_xlim(bounds)
ax.set_ylim(bounds)
# Ensure the aspect ratio is square
ax.set_aspect("equal", adjustable="box")

plt.plot(x,y,"o", alpha=0.5 ,ms=10, markeredgewidth=0.0)

ax.plot([0, 1], [0, 1], "r-",lw=2 ,transform=ax.transAxes)

# Calculate Statistics of the Parity Plot
mean_abs_err = np.mean(np.abs(x-y))
rmse = np.sqrt(np.mean((x-y)**2))
rmse_std = rmse / np.std(y)
z = np.polyfit(x,y, 1)
y_hat = np.poly1d(z)(x)

text = f"$\: \: Mean \: Absolute \: Error \: (MAE) = {mean_abs_err:0.3f}$ \n $ Root \: Mean \: Square \: Error \: (RMSE) = {rmse:0.3f}$ \n $ RMSE \: / \: Std(y) = {rmse_std :0.3f}$ \n $R^2 = {r2_score(y,y_hat):0.3f}$"

plt.gca().text(0.05, 0.95, text,transform=plt.gca().transAxes,
     fontsize=14, verticalalignment='top')

# Title and labels
plt.title("Parity Plot", fontdict=font_axis_publish)
plt.xlabel('Ground Truth', fontdict=font_axis_publish)
plt.ylabel('Prediction', fontdict=font_axis_publish)

# Save the figure into 300 dpi
fignow.savefig("parityplot.png",format = "png",dpi=300,bbox_inches='tight')

font_axis_publish = {
        'color':  'black',
        'weight': 'bold',
        'size': 18, 'fontname':'Serif'
        }
x = np.array(y_test)[7000:8000,3]
y = predictions[7000:8000,3]
## find the boundaries of X and Y values
bounds = (-0.01, 0.13)
# Reset the limits
ax = plt.gca()
ax.set_xlim(bounds)
ax.set_ylim(bounds)
ax.set_aspect('equal')
# fig, ax = plt.subplots()
# Ensure the aspect ratio is square
plt.plot(x,y,"o",markerfacecolor='g', ms=4, markeredgecolor='g')
plt.xticks(np.arange(0.002,0.012,0.004),fontsize=18)
plt.yticks(np.arange(0.002,0.012,0.004),fontsize=18)
ax.plot([0, 1], [0, 1], "m-",lw=2 ,transform=ax.transAxes)
params = {'mathtext.default': 'regular' }
plt.rcParams.update(params)
plt.xlabel('$r_{N}$ (True)',font_axis_publish)
plt.ylabel('$r_{N}$ (Predicted)',font_axis_publish)
plt.savefig('rn.eps',dpi=900,bbox_inches='tight')
plt.show()

from sklearn.metrics import mean_squared_error
f=mean_squared_error(x, y, squared=False)

f

